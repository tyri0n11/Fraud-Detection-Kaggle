{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b28de97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression, GBTClassifier, RandomForestClassifier\n",
    "\n",
    "from pyspark.ml import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be6525de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/14 09:35:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "26/01/14 09:35:08 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "26/01/14 09:35:08 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "26/01/14 09:35:08 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "26/01/14 09:35:08 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "26/01/14 09:35:08 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "26/01/14 09:35:08 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "26/01/14 09:35:08 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "26/01/14 09:35:08 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "26/01/14 09:35:08 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "26/01/14 09:35:08 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "26/01/14 09:35:08 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "26/01/14 09:35:08 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "26/01/14 09:35:08 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "26/01/14 09:35:08 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "26/01/14 09:35:08 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "26/01/14 09:35:08 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "26/01/14 09:35:08 ERROR SparkContext: Error initializing SparkContext.\n",
      "java.net.BindException: bind(..) failed with error(-49): Can't assign requested address: Service 'sparkDriver' failed after 16 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'sparkDriver' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.\n",
      "26/01/14 09:35:08 WARN SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor). This may indicate an error, since only one SparkContext should be running in this JVM (see SPARK-2243). The other SparkContext was created at:\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing SparkContext to stop: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n",
      ": java.net.BindException: bind(..) failed with error(-49): Can't assign requested address: Service 'sparkDriver' failed after 16 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'sparkDriver' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stop any existing SparkContext to clean up resources\n",
    "from pyspark import SparkContext\n",
    "import time\n",
    "\n",
    "try:\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    sc.stop()\n",
    "    time.sleep(2)  # Wait for Spark to clean up\n",
    "except Exception as e:\n",
    "    print(f\"No existing SparkContext to stop: {e}\")\n",
    "\n",
    "# Create a fresh SparkSession with proper configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Fraud Detection Modeling\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "input_path = \"../data/features/fraud_features_v2.parquet\"\n",
    "df_model = spark.read.parquet(input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0e39ce",
   "metadata": {},
   "source": [
    "### Data Spliting (Time-based split)\n",
    "\n",
    "Train: Jan â†’ May (80%)\n",
    "\n",
    "Validation: June (15%)\n",
    "\n",
    "Test: July (5%)\n",
    "\n",
    "The dataset spans from January 1, 2023 to July 2, 2023.\n",
    "To prevent temporal leakage, a time-based split was applied. Data from January to May was used for training, June for validation, and early July for final testing. This setup simulates a real-world fraud detection scenario where models are evaluated on future transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dd9433f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1447243\n",
      "Val:   287840\n",
      "Test:  19072\n"
     ]
    }
   ],
   "source": [
    "train_df = df_model.filter(col(\"TX_DATE\") < \"2023-06-01\")\n",
    "val_df   = df_model.filter((col(\"TX_DATE\") >= \"2023-06-01\") & (col(\"TX_DATE\") < \"2023-07-01\"))\n",
    "test_df  = df_model.filter(col(\"TX_DATE\") >= \"2023-07-01\")\n",
    "\n",
    "\n",
    "print(\"Train:\", train_df.count())\n",
    "print(\"Val:  \", val_df.count())\n",
    "print(\"Test: \", test_df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70d38678",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    "    \"TX_AMOUNT\",\n",
    "    \"LOG_TX_AMOUNT\",\n",
    "    \"TX_TIME_SECONDS\",\n",
    "    \"TX_TIME_DAYS\",\n",
    "    \"TX_HOUR\",\n",
    "    \"IS_NIGHT\",\n",
    "    \"IS_WEEKEND\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e411e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf384dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/14 09:35:14 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------+\n",
      "|TX_FRAUD|         probability|prediction|\n",
      "+--------+--------------------+----------+\n",
      "|       0|[0.99998825603487...|       0.0|\n",
      "|       0|[0.99999004502516...|       0.0|\n",
      "|       1|[0.00146348402120...|       1.0|\n",
      "|       1|[0.09028602988445...|       1.0|\n",
      "|       0|[0.99993244944999...|       0.0|\n",
      "+--------+--------------------+----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler(\n",
    "    inputCol=\"features\",\n",
    "    outputCol=\"scaled_features\",\n",
    "    withMean=True,\n",
    "    withStd=True\n",
    ")\n",
    "\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"scaled_features\",\n",
    "    labelCol=\"TX_FRAUD\"\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, scaler, lr])\n",
    "lr_model = pipeline.fit(train_df)\n",
    "val_pred = lr_model.transform(val_df)\n",
    "val_pred.select(\"TX_FRAUD\", \"probability\", \"prediction\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db0eec73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------+\n",
      "|TX_FRAUD|         probability|prediction|\n",
      "+--------+--------------------+----------+\n",
      "|       0|[0.95339405751833...|       0.0|\n",
      "|       0|[0.95339405751833...|       0.0|\n",
      "|       1|[0.04364652142729...|       1.0|\n",
      "|       1|[0.04364652142729...|       1.0|\n",
      "|       0|[0.95321574928095...|       0.0|\n",
      "+--------+--------------------+----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gbt = GBTClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"TX_FRAUD\",\n",
    ")\n",
    "\n",
    "gbt_pipeline = Pipeline(stages=[assembler, gbt])\n",
    "gbt_model = gbt_pipeline.fit(train_df)\n",
    "pred = gbt_model.transform(val_df)\n",
    "pred.select(\"TX_FRAUD\", \"probability\", \"prediction\").show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "654af269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------+\n",
      "|TX_FRAUD|         probability|prediction|\n",
      "+--------+--------------------+----------+\n",
      "|       0|[0.98640462558873...|       0.0|\n",
      "|       0|[0.98640462558873...|       0.0|\n",
      "|       1|           [0.0,1.0]|       1.0|\n",
      "|       1|           [0.0,1.0]|       1.0|\n",
      "|       0|[0.98640462558873...|       0.0|\n",
      "+--------+--------------------+----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "rf= RandomForestClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"TX_FRAUD\",\n",
    ")\n",
    "\n",
    "rf_pipeline = Pipeline(stages=[assembler, rf])\n",
    "rf_model = rf_pipeline.fit(train_df)\n",
    "\n",
    "rf_pred = rf_model.transform(val_df)\n",
    "rf_pred.select(\"TX_FRAUD\", \"probability\", \"prediction\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e133eb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models and predictions saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save models and predictions for evaluation notebook\n",
    "lr_model.write().overwrite().save(\"../models/lr_baseline_model\")\n",
    "gbt_model.write().overwrite().save(\"../models/gbt_baseline_model\")\n",
    "rf_model.write().overwrite().save(\"../models/rf_baseline_model\")\n",
    "# Save predictions (optional, can also recreate them in evaluation notebook)\n",
    "val_pred.write.mode('overwrite').parquet(\"../data/predictions/lr_predictions.parquet\")\n",
    "pred.write.mode('overwrite').parquet(\"../data/predictions/gbt_predictions.parquet\")\n",
    "rf_pred.write.mode('overwrite').parquet(\"../data/predictions/rf_predictions.parquet\")\n",
    "\n",
    "print(\"Models and predictions saved successfully!\")\n",
    "\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
