{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b28de97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression, GBTClassifier\n",
    "\n",
    "from pyspark.ml import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "be6525de",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Fraud Detection Modeling\").getOrCreate()\n",
    "\n",
    "input_path = \"../data/features/fraud_features_v2.parquet\"\n",
    "df_model = spark.read.parquet(input_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0e39ce",
   "metadata": {},
   "source": [
    "### Data Spliting (Time-based split)\n",
    "\n",
    "Train: Jan â†’ May (80%)\n",
    "\n",
    "Validation: June (15%)\n",
    "\n",
    "Test: July (5%)\n",
    "\n",
    "The dataset spans from January 1, 2023 to July 2, 2023.\n",
    "To prevent temporal leakage, a time-based split was applied. Data from January to May was used for training, June for validation, and early July for final testing. This setup simulates a real-world fraud detection scenario where models are evaluated on future transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9dd9433f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1447243\n",
      "Val:   287840\n",
      "Test:  19072\n"
     ]
    }
   ],
   "source": [
    "train_df = df_model.filter(col(\"TX_DATE\") < \"2023-06-01\")\n",
    "val_df   = df_model.filter((col(\"TX_DATE\") >= \"2023-06-01\") & (col(\"TX_DATE\") < \"2023-07-01\"))\n",
    "test_df  = df_model.filter(col(\"TX_DATE\") >= \"2023-07-01\")\n",
    "\n",
    "\n",
    "print(\"Train:\", train_df.count())\n",
    "print(\"Val:  \", val_df.count())\n",
    "print(\"Test: \", test_df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "70d38678",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    "    \"TX_AMOUNT\",\n",
    "    \"LOG_TX_AMOUNT\",\n",
    "    \"TX_TIME_SECONDS\",\n",
    "    \"TX_TIME_DAYS\",\n",
    "    \"TX_HOUR\",\n",
    "    \"IS_NIGHT\",\n",
    "    \"IS_WEEKEND\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8e411e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cf384dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(\n",
    "    inputCol=\"features\",\n",
    "    outputCol=\"scaled_features\",\n",
    "    withMean=True,\n",
    "    withStd=True\n",
    ")\n",
    "\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"scaled_features\",\n",
    "    labelCol=\"TX_FRAUD\"\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, scaler, lr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2644e287",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------+\n",
      "|TX_FRAUD|         probability|prediction|\n",
      "+--------+--------------------+----------+\n",
      "|       0|[0.99978536182453...|       0.0|\n",
      "|       1|[0.24325566066173...|       1.0|\n",
      "|       0|[0.99999033257916...|       0.0|\n",
      "|       0|[0.99998703625320...|       0.0|\n",
      "|       0|[0.95695074777610...|       0.0|\n",
      "+--------+--------------------+----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "lr_model = pipeline.fit(train_df)\n",
    "val_pred = lr_model.transform(val_df)\n",
    "val_pred.select(\"TX_FRAUD\", \"probability\", \"prediction\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "db0eec73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "gbt = GBTClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"TX_FRAUD\",\n",
    "    maxDepth=4,        \n",
    "    maxIter=20,        # default 20\n",
    "    subsamplingRate=0.8,\n",
    "    stepSize=0.1\n",
    ")\n",
    "\n",
    "gbt_pipeline = Pipeline(stages=[assembler, gbt])\n",
    "gbt_model = gbt_pipeline.fit(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ba393906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------+\n",
      "|TX_FRAUD|         probability|prediction|\n",
      "+--------+--------------------+----------+\n",
      "|       0|[0.95325922538474...|       0.0|\n",
      "|       1|[0.04364652142728...|       1.0|\n",
      "|       0|[0.95343301850236...|       0.0|\n",
      "|       0|[0.95343301850236...|       0.0|\n",
      "|       0|[0.95317046835999...|       0.0|\n",
      "+--------+--------------------+----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "pred = gbt_model.transform(val_df)\n",
    "pred.select(\"TX_FRAUD\", \"probability\", \"prediction\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e133eb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models and predictions saved successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save models and predictions for evaluation notebook\n",
    "lr_model.write().overwrite().save(\"../models/lr_model\")\n",
    "gbt_model.write().overwrite().save(\"../models/gbt_model\")\n",
    "\n",
    "# Save predictions (optional, can also recreate them in evaluation notebook)\n",
    "val_pred.write.mode('overwrite').parquet(\"../data/predictions/lr_predictions.parquet\")\n",
    "pred.write.mode('overwrite').parquet(\"../data/predictions/gbt_predictions.parquet\")\n",
    "\n",
    "print(\"Models and predictions saved successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
